{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"EM-Transformer-Example.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"cells":[{"cell_type":"code","metadata":{"id":"KWfHadLDGATp"},"source":["# Installer nødvendige pakker før vi starter\n","# Legg merke til at alle de \"vanlige\" pakkene er forhåndsinstallerte\n","!pip install transformers pytorch-lightning sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5vBuYDkPGRx6"},"source":["import pandas as pd\n","import numpy as np\n","from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n","import pytorch_lightning as pl\n","from torch.utils.data import Dataset, DataLoader\n","from multiprocessing import cpu_count\n","from torch.cuda.amp import autocast\n","from torch import nn\n","import torch.nn.functional as F\n","import torch\n","from tqdm import tqdm_notebook\n","from sentence_transformers import SentenceTransformer, InputExample, SentencesDataset, losses, models\n","from sentence_transformers.evaluation import TripletEvaluator, SimilarityFunction\n","from sentence_transformers.util import pytorch_cos_sim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1m9o9JdFjnnK"},"source":["# Google gir oss en tilfeldig GPU\n","# Typisk T4, V100, P100 eller K80\n","# T4 og V100 er svært raske kan kjøre 16bit operasjoner raskt\n","# K80 kan være håpløs trege for de tyngste nettverkene\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M48w6bIcFLbY"},"source":["# Config"]},{"cell_type":"markdown","metadata":{"id":"DAr51SFamBiE"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"FgRZ_wmTmDe5"},"source":["# Dere kan fikse datasett slik dere selv ønsker det, men her litt kode for å laste ned noen vanlige datasett\n","\n","import os\n","import urllib.request\n","import logging\n","from functools import lru_cache\n","import zipfile\n","from glob import glob\n","import pandas as pd\n","\n","log = logging.getLogger(__name__)\n","\n","\n","class EntityMatchingDataset:\n","    def __init__(self, name, data_dir = None):\n","        self.name = name\n","        if not data_dir:\n","            data_dir = os.path.join(os.getcwd(), \"data\")\n","        self.data_dir = data_dir\n","    \n","    @property\n","    @lru_cache(maxsize=1)\n","    def records_a(self):\n","        self.download()\n","        return self._load(\"records_a\")\n","    \n","    @property\n","    @lru_cache(maxsize=1)\n","    def records_b(self):\n","        self.download()\n","        return self._load(\"records_b\")\n","    \n","    @property\n","    @lru_cache(maxsize=1)\n","    def matches_train(self):\n","        self.download()\n","        return self._load(\"matches_train\")\n","    \n","    @property\n","    @lru_cache(maxsize=1)\n","    def matches_val(self):\n","        self.download()\n","        return self._load(\"matches_val\")\n","    \n","    @property\n","    @lru_cache(maxsize=1)\n","    def matches_test(self):\n","        self.download()\n","        return self._load(\"matches_test\")\n","    \n","    def _download_file(self, url, filename):\n","        file_path = os.path.join(self.data_dir, self.name, filename)\n","        if not os.path.exists(file_path):\n","            log.warning(f\"Downloading {url} to {file_path}\")\n","            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","            urllib.request.urlretrieve(url, file_path)\n","    \n","    def download(self):\n","        ...\n","        \n","    def load(self):\n","        self.records_a\n","        self.records_b\n","        self.matches_train\n","        self.matches_val\n","        self.matches_test\n","        \n","class DeepMatcherDataset(EntityMatchingDataset):\n","    def __init__(self, name, url, data_dir = None):\n","        super().__init__(name, data_dir)\n","        self._url = url\n","    \n","    def download(self):\n","        self._download_file(urllib.parse.urljoin(self._url, \"tableA.csv\"), \"tableA.csv\")\n","        self._download_file(urllib.parse.urljoin(self._url, \"tableB.csv\"), \"tableB.csv\")\n","        self._download_file(urllib.parse.urljoin(self._url, \"train.csv\"), \"train.csv\")\n","        self._download_file(urllib.parse.urljoin(self._url, \"valid.csv\"), \"valid.csv\")\n","        self._download_file(urllib.parse.urljoin(self._url, \"test.csv\"), \"test.csv\")\n","    \n","    def _load(self, t):\n","        filename = {\n","            \"records_a\": \"tableA.csv\",\n","            \"records_b\": \"tableB.csv\",\n","            \"matches_train\": \"train.csv\",\n","            \"matches_val\": \"valid.csv\",\n","            \"matches_test\": \"test.csv\",\n","        }[t]\n","        if t.startswith(\"records\"):\n","            return pd.read_csv(os.path.join(self.data_dir, self.name, filename), index_col=\"id\").rename_axis(index=\"index\")\n","        else:\n","            return pd.read_csv(os.path.join(self.data_dir, self.name, filename)).rename(columns={\"ltable_id\": \"a.index\", \"rtable_id\": \"b.index\", \"label\": \"matching\"}).astype({\"matching\": \"bool\"})\n","\n","def deepmatcher_structured_amazon_google(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/Amazon-Google\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Amazon-Google/exp_data/\", data_dir)\n","        \n","def deepmatcher_structured_beer(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/Beer\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Beer/exp_data/\", data_dir)\n","\n","def deepmatcher_structured_dblp_acm(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/DBLP-ACM\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/DBLP-ACM/exp_data/\", data_dir)\n","\n","def deepmatcher_structured_dblp_google_scholar(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/DBLP-GoogleScholar\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/DBLP-GoogleScholar/exp_data/\", data_dir)\n","    \n","def deepmatcher_structured_fodors_zagats(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/Fodors-Zagats\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Fodors-Zagats/exp_data/\", data_dir)\n","    \n","def deepmatcher_structured_walmart_amazon(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/Walmart-Amazon\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/Walmart-Amazon/exp_data/\", data_dir)\n","    \n","def deepmatcher_structured_itunes_amazon(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Structured/iTunes-Amazon\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Structured/iTunes-Amazon/exp_data/\", data_dir)\n","\n","    \n","def deepmatcher_dirty_dblp_acm(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Dirty/DBLP-ACM\" ,\"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Dirty/DBLP-ACM/exp_data/\", data_dir)\n","\n","def deepmatcher_dirty_dblp_google_scholar(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Dirty/DBLP-GoogleScholar\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Dirty/DBLP-GoogleScholar/exp_data/\", data_dir)\n","    \n","def deepmatcher_dirty_walmart_amazon(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Dirty/Walmart-Amazon\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Dirty/Walmart-Amazon/exp_data/\", data_dir)\n","    \n","def deepmatcher_dirty_itunes_amazon(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Dirty/iTunes-Amazon\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Dirty/iTunes-Amazon/exp_data/\", data_dir)\n","\n","    \n","def deepmatcher_textual_abt_buy(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Textual/Abt-Buy\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Textual/Abt-Buy/exp_data/\", data_dir)\n","    \n","def deepmatcher_textual_company(data_dir=None):\n","    return DeepMatcherDataset(\"DeepMatcher/Textual/Company\", \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/Textual/Company/exp_data/\", data_dir)\n","\n","\n","class CompERBenchDataset(EntityMatchingDataset):\n","    def __init__(self, name, url, deduplication=False, data_dir = None):\n","        super().__init__(name, data_dir)\n","        self._url = url\n","        self._deduplication = deduplication\n","    \n","    def download(self):\n","        self._download_file(urllib.parse.urljoin(self._url, \"records.zip\"), \"records.zip\")\n","        if not os.path.exists(os.path.join(self.data_dir, self.name, \"record_descriptions\")):\n","            with zipfile.ZipFile(os.path.join(self.data_dir, self.name, \"records.zip\")) as zip_ref:\n","                zip_ref.extractall(os.path.join(self.data_dir, self.name))\n","        self._download_file(urllib.parse.urljoin(self._url, \"gs_train.csv\"), \"gs_train.csv\")\n","        self._download_file(urllib.parse.urljoin(self._url, \"gs_val.csv\"), \"gs_val.csv\")\n","        self._download_file(urllib.parse.urljoin(self._url, \"gs_test.csv\"), \"gs_test.csv\")\n","    \n","    def _load(self, t):\n","        path = {\n","            \"records_a\": glob(os.path.join(self.data_dir, self.name, \"record_descriptions\", \"1_*\"))[0],\n","            \"records_b\": glob(os.path.join(self.data_dir, self.name, \"record_descriptions\", \"2_*\" if not self._deduplication else \"1_*\"))[0],\n","            \"matches_train\": os.path.join(self.data_dir, self.name, \"gs_train.csv\"),\n","            \"matches_val\": os.path.join(self.data_dir, self.name, \"gs_val.csv\"),\n","            \"matches_test\": os.path.join(self.data_dir, self.name, \"gs_test.csv\"),\n","        }[t]\n","        if t.startswith(\"records\"):\n","            return pd.read_csv(path, index_col=\"subject_id\", encoding=\"iso-8859-1\").rename_axis(index=\"index\")\n","        else:\n","            return pd.read_csv(path, encoding=\"iso-8859-1\").rename(columns={\"source_id\": \"a.index\", \"target_id\": \"b.index\"}).astype({\"matching\": bool})\n","        \n","def comperbench_abt_buy(data_dir=None):\n","    return CompERBenchDataset(\"CompERBench/abt-buy\", \"http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/data/abt-buy/\", data_dir)\n","\n","def comperbench_wdc_xlarge_shoes(data_dir=None):\n","    return CompERBenchDataset(\"CompERBench/wdc_xlarge_shoes\", \"http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/data/wdc_xlarge_shoes/\", deduplication=True, data_dir=data_dir)\n","\n","class LocalParquetDataset(EntityMatchingDataset):\n","    def __init__(self, name, data_dir = None):\n","        super().__init__(name, data_dir)\n","    \n","    def download(self):\n","        ...\n","        \n","    def _load(self, t):\n","        return pd.read_parquet(os.path.join(self.data_dir, self.name, t + \".parquet\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2fQUi51VhO-"},"source":["d = deepmatcher_textual_abt_buy()\n","d.load()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QScA71jUO_o"},"source":["# La oss se på noen av de positive matchene i treningsdataene\n","d.matches_train.query(\"matching\").merge(d.records_a, left_on=\"a.index\", right_index=True).merge(d.records_b, left_on=\"b.index\", right_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkIFgktNPBRB"},"source":["# Det er kjekt å ha enkelt tilgang til den formaterte streng-varianten av alle records senere\n","records_a = format_records(d.records_a)\n","records_b = format_records(d.records_b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8U3e4OV3FmLA"},"source":["# Blocking"]},{"cell_type":"code","metadata":{"id":"kkI5S-lHGdaq"},"source":["# Veldig stor model\n","# bert_model = \"roberta-base\"\n","\n","# En litt mindre modell som er ganske populær\n","#bert_model = \"distilbert-base-cased\"\n","\n","# Google har en haug med modeller i alle tenkelige størrelser\n","# Her er noen i synkende størrelse - presisjonen faller hele veien\n","# De kan være fine å eksperimentere med noen ganger for å teste ting fortere\n","#bert_model = \"google/bert_uncased_L-8_H-512_A-8\"\n","bert_model = \"google/bert_uncased_L-4_H-512_A-8\"\n","#bert_model = \"google/bert_uncased_L-4_H-256_A-4\"\n","#bert_model = \"google/bert_uncased_L-2_H-128_A-2\"\n","\n","# Det finnes ingen resultater som sier noe om ulike transformer-modeller for blocking.\n","# Vi tar en mindre en for blocking for å spare tid - egen erfaring tilsier at veldig store modeller gir mye mindre avkastning for blocking enn matching"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5JjOt4_DV_IQ"},"source":["### Data preprocessing and loading"]},{"cell_type":"code","metadata":{"id":"q5EI0eqOr4nf"},"source":["# For blocking trener vi ved hjelp av triplet loss.\n","# Altså istedenfor at loss'et reflekterer hvor like to embeddings skal være\n","# reflekterer det heller at vi vil at en embedding skal være mer lik en annen\n","# enn en tredje\n","\n","def get_triplet_examples(records_a, records_b, matches):\n","    records_a = format_records(records_a)\n","    records_b = format_records(records_b)\n","\n","    matches = matches.merge(records_a.to_frame(\"a.val\"), left_on=\"a.index\", right_index=True)\n","    matches = matches.merge(records_b.to_frame(\"b.val\"), left_on=\"b.index\", right_index=True)\n","\n","    positives = matches[matches[\"matching\"]]\n","    negatives = matches[~matches[\"matching\"]]\n","\n","    triplets_a = positives.merge(negatives[[\"a.index\", \"b.val\"]].rename(columns={\"b.val\": \"neg.val\"}), on=\"a.index\")[[\"a.val\", \"b.val\", \"neg.val\"]]\n","    triplets_b = positives.merge(negatives[[\"b.index\", \"a.val\"]].rename(columns={\"a.val\": \"neg.val\"}), on=\"b.index\")[[\"b.val\", \"a.val\", \"neg.val\"]]\n","    return list(InputExample(texts=t) for t in triplets_a.itertuples(index=False, name=None)) + list(InputExample(texts=t) for t in triplets_b.itertuples(index=False, name=None))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMqAumP5WKaa"},"source":["input_examples = get_triplet_examples(d.records_a, d.records_b, d.matches_train)\n","val_input_examples = get_triplet_examples(d.records_a, d.records_b, d.matches_val)\n","\n","train_dataset = SentencesDataset(input_examples, m)\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFl4OAhMWQKZ"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"EwvpsZ-7WBZk"},"source":["# Modellen vår produserer en embedding for et helt record vet å ta gjennomsnitt\n","# over embeddingene til alle tokens.\n","# SentenceTransformer-biblioteket tilbyr alt vi trenger.\n","\n","word_embedding_model = models.Transformer(bert_model, max_seq_length=256)\n","pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n","\n","m = SentenceTransformer(modules=[word_embedding_model, pooling_model])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkOEioDhWRx_"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"5wiNZVrQWDan"},"source":["# Det er masse hyperparametere som kan tunes, men disse er ganske gode default\n","# som jeg selv har funnet.\n","# Det meste relevante å endre er antall epochs.\n","# For vanskelige data eller hvis det er lite treningsdata kan man øke epochs noe,\n","# men hvis dataene er enkle og det er masse treningsdata kan man vurdere å senke\n","# antall epochs for å spare tid.\n","\n","# Validerings-loss spyttes ut hver epoch - dette kan gi en indikasjon om nettverket overfitter. (Dette funker plutselig ikke?)\n","\n","import logging\n","logging.getLogger().setLevel(logging.INFO) # Se validerings-loss\n","\n","train_loss = losses.TripletLoss(m, distance_metric=losses.TripletDistanceMetric.COSINE, triplet_margin=0.1)\n","evaluator = TripletEvaluator([e.texts[0] for e in val_input_examples], [e.texts[1] for e in val_input_examples], [e.texts[2] for e in val_input_examples], main_distance_function=SimilarityFunction.COSINE)\n","m.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator, use_amp=True, evaluation_steps=len(train_dataloader))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FH5fyXIbWUVn"},"source":["### Testing"]},{"cell_type":"code","metadata":{"id":"blCjArpWs88s"},"source":["# Vi starter med å finne embeddingen for alle records\n","\n","embeddings_a = m.encode(records_a, batch_size=128)\n","embeddings_b = m.encode(records_b, batch_size=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yh-51YKfcypf"},"source":["# La oss se hva recall og reduction rate blir hvis vi plukker ut de k nærmeste\n","# recordene for alle records i A og B\n","\n","for k in [1, 5, 10, 20, 50, 100]:\n","  cos_scores = pytorch_cos_sim(embeddings_a, embeddings_b)\n","  cos_scores = cos_scores.cpu()\n","\n","  blocked_matches = set()\n","\n","  topk_sims, topk = torch.topk(cos_scores, k=k, dim=1)\n","  for i in range(topk.shape[0]):\n","    for j in range(topk.shape[1]):\n","      blocked_matches.add((i, topk[i, j].item()))\n","\n","  topk_sims, topk = torch.topk(cos_scores, k=k, dim=0)\n","  for i in range(topk.shape[0]):\n","    for j in range(topk.shape[1]):\n","      blocked_matches.add((topk[i, j].item(), j))\n","\n","  gold_matches = set(d.matches_test[d.matches_test[\"matching\"]][[\"a.index\", \"b.index\"]].itertuples(index=False, name=None))\n","\n","  recall = len(gold_matches & blocked_matches) / len(gold_matches)\n","  reduction_rate = 1 - len(blocked_matches) / (len(records_a)*len(records_b))\n","\n","  print(k, recall, reduction_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36kfIn_pGGMY"},"source":["# Matching"]},{"cell_type":"code","metadata":{"id":"3OzlaIskZaP7"},"source":["# Veldig stor model - fungerer stort sett best blant modeller av tilsvarende størrelse og gir omtrentlig SOTA\n","# Større modeller finnes, men de blir gjerne håpløst uhåndterbare for forsvinnende liten gevinst\n","bert_model = \"roberta-base\"\n","\n","# En litt mindre modell som er ganske populær\n","#bert_model = \"distilbert-base-cased\"\n","\n","# Google har en haug med modeller i alle tenkelige størrelser\n","# Her er noen i synkende størrelse - presisjonen faller hele veien\n","# De kan være fine å eksperimentere med noen ganger for å teste ting fortere\n","#bert_model = \"google/bert_uncased_L-8_H-512_A-8\"\n","#bert_model = \"google/bert_uncased_L-4_H-512_A-8\"\n","#bert_model = \"google/bert_uncased_L-4_H-256_A-4\"\n","#bert_model = \"google/bert_uncased_L-2_H-128_A-2\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3lUpa8yVqIx"},"source":["### Data preprocessing and loading"]},{"cell_type":"code","metadata":{"id":"WL5PENudx-zq"},"source":["# Vi må konvertere records til et passende format for et transformer nettverk\n","# og sette opp nødvendig infrastruktur for å la PyTorch-verktøy laste inn\n","# batches med eksempler\n","\n","# Vi gjør som Ditto og formaterer et record som:\n","\n","# COL name VAL et navn COL description VAL en beskrivelse\n","\n","def format_records(records):\n","    return records.apply(lambda r: \" \".join(f\"COL {c} VAL {v}\" for c, v in r.iteritems()), axis=1)\n","\n","class BertEntityMatchingDataset(Dataset):\n","    def __init__(self, records_a, records_b, matches, bert_model=None, tokenizer=None):\n","        assert bert_model is not None or tokenizer is not None\n","\n","        self.records_a = records_a\n","        self.records_b = records_b\n","        self.matches = matches\n","        if tokenizer:\n","          self.tokenizer = tokenizer\n","        else:\n","          self.tokenizer = AutoTokenizer.from_pretrained(bert_model, use_fast=True)\n","        \n","        if \"matching\" in matches.columns:\n","          self._labels = matches[\"matching\"]\n","        else:\n","          self._labels = None\n","\n","        matches = matches[[\"a.index\", \"b.index\"]]\n","\n","        matches_a = matches.merge(records_a, how=\"left\", left_on=\"a.index\", right_index=True).drop(columns=[\"a.index\", \"b.index\"])\n","        matches_b = matches.merge(records_b, how=\"left\", left_on=\"b.index\", right_index=True).drop(columns=[\"a.index\", \"b.index\"])\n","        matches_a = format_records(matches_a).tolist()\n","        matches_b = format_records(matches_b).tolist()\n","\n","        self._encoded_pairs = self.tokenizer(matches_a, matches_b, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","\n","    def __len__(self):\n","        return len(self.matches)\n","    \n","    def __getitem__(self, index):\n","        encoded_pair = {k: v[index] for k, v in self._encoded_pairs.items()}\n","        \n","        return encoded_pair, float(self._labels.iloc[index])\n","\n","\n","class BertEntityMatchingDataModule(pl.LightningDataModule):\n","    def __init__(self, dataset, bert_model, train_batch_size=64, inference_batch_size=128, num_workers=None):\n","        super().__init__()\n","        self._dataset = dataset\n","        self.bert_model = bert_model\n","        self.train_batch_size = train_batch_size\n","        self.inference_batch_size = inference_batch_size\n","        if num_workers is None:\n","            num_workers = cpu_count()\n","        self.num_workers = num_workers\n","    \n","    def prepare_data(self):\n","        self._dataset.download()\n","    \n","    def setup(self, stage = None):\n","        self._train = BertEntityMatchingDataset(self._dataset.records_a, self._dataset.records_b, self._dataset.matches_train, self.bert_model)\n","        self._val = BertEntityMatchingDataset(self._dataset.records_a, self._dataset.records_b, self._dataset.matches_val, self.bert_model)\n","        self._test = BertEntityMatchingDataset(self._dataset.records_a, self._dataset.records_b, self._dataset.matches_test, self.bert_model)\n","        \n","    def train_dataloader(self):\n","        return DataLoader(self._train, batch_size=self.train_batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n","    \n","    def val_dataloader(self):\n","        return DataLoader(self._val, batch_size=self.inference_batch_size, num_workers=self.num_workers, pin_memory=True)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self._test, batch_size=self.inference_batch_size, num_workers=self.num_workers, pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EMiLzPkVIE_y"},"source":["data_module = BertEntityMatchingDataModule(\n","    d,\n","    bert_model,\n","    num_workers=0 if not torch.cuda.is_available() else None,\n","    train_batch_size=32,\n","    inference_batch_size=32\n",")\n","data_module.prepare_data()\n","data_module.setup()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZcWIbV4CGIuD"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"GcE8-Yymfasq"},"source":["# Vi må definere nettverket vårt.\n","# Her er det gitt ved _Model, som legger til et enkelt lag på utsiden av\n","# et forhåndstrent transformer-nettverk\n","\n","# I tillegg lager jeg her en Lightning-modul som sier hvordan vi ønsker å trene\n","# nettverket, validere og test.\n","# Man trenger ikke bruke PyTorch Ligthning, man kan også ganske enkelt skrive\n","# nødvendig logikk selv, slike moduler bare gjør det litt mer oversiktlig\n","\n","class _Model(nn.Module):\n","    def __init__(self, bert_model):\n","        super().__init__()\n","        self.bert = AutoModel.from_pretrained(bert_model)\n","        self.dropout = nn.Dropout(0.1)\n","        self.cls_layer = nn.Linear(self.bert.config.hidden_size, 1)\n","    \n","    def forward(self, encoded_input):\n","        x = self.bert(**encoded_input)\n","        x = self.cls_layer(self.dropout(x[0][:, 0]))\n","        return x\n","\n","class EntityMatchingModel(pl.LightningModule):\n","    def __init__(self, bert_model, lr=3e-5):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.model = _Model(bert_model)\n","\n","    def forward(self, x):\n","        return self.model(x).squeeze(dim=1)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","\n","        max_length = x[\"attention_mask\"].detach().sum(axis=1).max().item()\n","        x = {k: v[:, :max_length] for k, v in x.items()}\n","\n","        output = self(x)\n","        loss = nn.functional.binary_cross_entropy_with_logits(output, y)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n","        data_loader = self.train_dataloader()\n","        steps = self.trainer.max_steps if self.trainer.max_steps else self.trainer.max_epochs*len(data_loader)\n","        lr_scheduler = {\n","            \"scheduler\": torch.optim.lr_scheduler.OneCycleLR(\n","                optimizer,\n","                max_lr=self.hparams.lr,\n","                total_steps=steps//self.trainer.accumulate_grad_batches, anneal_strategy=\"linear\",\n","                div_factor=100,\n","                pct_start=0.05\n","            ),\n","            \"interval\": \"step\"\n","        }\n","        return [optimizer], [lr_scheduler]\n","\n","    def _test_batch(self, batch):\n","        x, y = batch\n","        max_length = x[\"attention_mask\"].detach().sum(axis=1).max().item()\n","        x = {k: v[:, :max_length] for k, v in x.items()}\n","        output = self(x)\n","        with autocast(enabled=False):\n","            y_hat = torch.sigmoid(output.type(torch.FloatTensor))\n","            loss = nn.functional.binary_cross_entropy(y_hat, y.type(torch.FloatTensor))\n","            predictions = torch.round(y_hat).type(torch.LongTensor)\n","            y = y.type(torch.LongTensor)\n","        return predictions, y, loss\n","\n","    def _test_epoch(self, step_outputs):\n","        p = 0\n","        tp = 0\n","        fn = 0\n","        for pred, y in step_outputs:\n","            y = y.type(torch.LongTensor)\n","            p += pred.sum().item()\n","            tp += (y & pred).sum().item()\n","            fn += (y & (~pred)).sum().item()\n","        precision = tp / p if p > 0 else 0\n","        recall = tp / (tp + fn) if tp + fn > 0 else 0\n","        f1 = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else 0\n","        return precision, recall, f1\n","\n","    def validation_step(self, batch, batch_idx):\n","        pred, y, loss = self._test_batch(batch)\n","        self.log('val_loss', loss, prog_bar=True)\n","        return pred, y\n","\n","    def validation_epoch_end(self, validation_step_outputs):\n","        precision, recall, f1 = self._test_epoch(validation_step_outputs)\n","        self.log(\"val_prec\", precision, prog_bar=True)\n","        self.log(\"val_recall\", recall, prog_bar=True)\n","        self.log(\"val_f1\", f1, prog_bar=True)\n","\n","    def test_step(self, batch, batch_idx):\n","        pred, y, loss = self._test_batch(batch)\n","        self.log('test_loss', loss, prog_bar=True)\n","        return pred, y\n","\n","    def test_epoch_end(self, test_step_outputs):\n","        precision, recall, f1 = self._test_epoch(test_step_outputs)\n","        self.log(\"test_prec\", precision)\n","        self.log(\"test_recall\", recall)\n","        self.log(\"test_f1\", f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqYfcGEOGa0a"},"source":["m = EntityMatchingModel(bert_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tYXQu2RoGYsb"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"7gubSmY3IGlt"},"source":["# Vi må trene modellen vår. Det er en haug med hyperparametere som kan stilles på,\n","# men disse er ganske gode defaults. Mest relevant er det kanskje å stille på\n","# antall epochs - noen ganger trenger man kanskje 5 eller 10.\n","\n","# 16-bit presisjon er først og fremst for å utnytte muligheter for økt ytelse på\n","# moderne GPUer med støtte for raske 16-bit operasjoner.\n","# Det påvirker vanligvis ikke kvaliteten på resultatet fordi et smart bibliotek\n","# bak gardinene velger hvilke operasjoner som skal være 16-bit og ikke.\n","\n","# Legg merke til at treningsrutinen spytter ut validerings-resultater etter hver epoch\n","\n","trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else None, max_epochs=3, precision=16 if torch.cuda.is_available() else 32, checkpoint_callback=False, check_val_every_n_epoch=1)\n","trainer.fit(m, data_module)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvg7fnmuLgNS"},"source":["### Testing"]},{"cell_type":"code","metadata":{"id":"z4u3hKVk30-q"},"source":["# Evaluer på testdataene.\n","# Dette gjør du aldri underveis i utviklingen av metoden,\n","# bare som et aller siste trinn for å få de endelige resultatene\n","# å putte inn i masteroppgaven/paperet.\n","# Bruk valideringsdataene istedenfor underveis.\n","trainer.test(m, datamodule=data_module)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uadc0J_zLrfr"},"source":["### Manual inference\n","\n","Bare for å illustrere litt hva som skjer legger jeg til et eksempel på å\n","manuelt mate inn et par vi vil klassifisere"]},{"cell_type":"code","metadata":{"id":"uwLtC9UqLrui"},"source":["# La oss ta en positive match fra valideringsdataene\n","record_pair = d.matches_val.query(\"matching\").iloc[0]\n","record_pair"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IgYJW_TBOLFa"},"source":["record_a = records_a.loc[record_pair[\"a.index\"]]\n","record_b = records_b.loc[record_pair[\"b.index\"]]\n","\n","print(record_a)\n","print(record_b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVnpGVokOLH8"},"source":["# Før vi mater det inn i nettverket må vi tokenize det på korrekt måte,\n","# pad'e korrekt før og mellom recordene og oversette hver token til en unik id\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","encoded_input = tokenizer([record_a], [record_b], return_tensors=\"pt\")\n","encoded_input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4O1pqEnRjn2"},"source":["# Vi kan se på den tekstlige representasjonen av tokens\n","\" \".join(tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_jifcgnOLK6"},"source":["# Når vi gjør det manuelt må vi passe på å flytte all input til GPUen\n","encoded_input = {k: v.to(\"cuda\") for k, v in encoded_input.items()}\n","\n","torch.sigmoid(m(encoded_input))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dC5reCuNLixd"},"source":["### Save & Load"]},{"cell_type":"code","metadata":{"id":"WkpeFt0kU53d"},"source":["# Det går an å lagre modellen til fil\n","# Dette er først og fremst aktuelt hvis du må restarte runtime (frivillig eller ufrivillig)\n","# så du slipper å trene igjen. Filer overlever runtime restart, men dette er ikke det samme som å avslutte sesjonen.\n","# Ikke sløs bort tid på å laste ned og opp modeller fra egen maskin mellom sesjoner.\n","# Filene er så store at det tar kortere tid å trene en modell igjen.\n","trainer.save_checkpoint(\"model.ckpt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ApHaJuBLrfCy"},"source":["# Laste modellen fra fil\n","m = EntityMatchingModel.load_from_checkpoint(\"model.ckpt\", bert_model=bert_model, lr=3e-5)\n","m = m.to(\"cuda\")"],"execution_count":null,"outputs":[]}]}